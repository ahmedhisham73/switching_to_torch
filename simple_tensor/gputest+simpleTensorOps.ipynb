{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b06dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.10.1\n",
      "\n",
      "Python 3.7.11 (default, Jul 27 2021, 14:32:16) \n",
      "[GCC 7.5.0]\n",
      "Pandas 1.3.5\n",
      "Scikit-Learn 1.0.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8713e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51c72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ed0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02490048",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304b1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 260.4604797363281\n",
      "199 175.35650634765625\n",
      "299 119.04248046875\n",
      "399 81.77507019042969\n",
      "499 57.11119079589844\n",
      "599 40.78739929199219\n",
      "699 29.982818603515625\n",
      "799 22.830890655517578\n",
      "899 18.096446990966797\n",
      "999 14.962103843688965\n",
      "1099 12.886877059936523\n",
      "1199 11.512773513793945\n",
      "1299 10.602783203125\n",
      "1399 10.000144958496094\n",
      "1499 9.601006507873535\n",
      "1599 9.3366060256958\n",
      "1699 9.161437034606934\n",
      "1799 9.04538631439209\n",
      "1899 8.968481063842773\n",
      "1999 8.917506217956543\n",
      "Result: y = -0.0021989361848682165 + 0.8472151756286621 x + 0.00037935248110443354 x^2 + -0.09197544306516647 x^3\n"
     ]
    }
   ],
   "source": [
    "#forward pass\n",
    "for t in range (2000):\n",
    "    y_predict=a + b * x + c * x ** 2 + d * x ** 3\n",
    "    #compute loss \n",
    "    loss = (y_predict - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    #backward pass\n",
    "    grad_y_predict = 2.0 * (y_predict - y)\n",
    "    grad_a = grad_y_predict.sum()\n",
    "    grad_b = (grad_y_predict * x).sum()\n",
    "    grad_c = (grad_y_predict * x ** 2).sum()\n",
    "    grad_d = (grad_y_predict * x ** 3).sum()\n",
    "    \n",
    "    #updating gradients using gradient descent\n",
    "        \n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51e490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ahisham/Documents/pytorch_endtoend/simple_tensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68eb271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d92d17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebebed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model mathematical function \n",
    "#y=a+bP3(c+dx)\n",
    "#where P3(x)=1/2(5x^3-3x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d28fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80590bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "903c390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35c51f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create random tensors \n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44dded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf0023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 nan\n",
      "199 nan\n",
      "299 nan\n",
      "399 nan\n",
      "499 nan\n",
      "599 nan\n",
      "699 nan\n",
      "799 nan\n",
      "899 nan\n",
      "999 nan\n",
      "1099 nan\n",
      "1199 nan\n",
      "1299 nan\n",
      "1399 nan\n",
      "1499 nan\n",
      "1599 nan\n",
      "1699 nan\n",
      "1799 nan\n",
      "1899 nan\n",
      "1999 nan\n",
      "Result: y = nan + nan * P3(nan + nan x)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "     # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecb78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
